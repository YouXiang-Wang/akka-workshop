akka {
  stdout-loglevel = "INFO"
  loglevel = "INFO"
  log-dead-letters = on

  actor {
    provider = cluster

    enable-additional-serialization-bindings = on
    allow-java-serialization = off

    #serializers {
      # kryo = "com.twitter.chill.akka.AkkaSerializer"
    #}

    #serialization-bindings {
      #"java.io.Serializable" = kryo
    #}

  }

  cluster {
    min-nr-of-members = 1
    log-info = on
    shutdown-after-unsuccessful-join-seed-nodes = 60s
    split-brain-resolver {
      active-strategy = keep-majority
      stable-after = 10s
    }
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  coordinated-shutdown.exit-jvm = on

  extensions = [
    "akka.cluster.pubsub.DistributedPubSub"
  ]

  remote {
    maximum-payload-bytes = 30000000 bytes
    netty.tcp {
      message-frame-size = 30000000b
      send-buffer-size = 30000000b
      receive-buffer-size = 30000000b
      maximum-frame-size = 30000000b
    }

    artery {
      advanced {
        maximum-frame-size = 512KiB
        buffer-pool-size = 128
        maximum-large-frame-size = 4MiB
        large-buffer-pool-size = 32
      }
    }
  }

}

# akka.http.server.websocket.periodic-keep-alive-max-idle = 10 second
# akka.cluster.sharding.remember-entities = on
# 1 s 1 m 1 h 1 d
# akka.cluster.sharding.passivate-idle-entity-after = 2 m
akka.cluster.sharding.passivate-idle-entity-after = 168 hours

executor {
  persistence-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 4
      parallelism-factor = 1
      parallelism-max = 32
    }
    throughput = 100
  }

  service-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 8
      parallelism-factor = 24.0
      parallelism-max = 64
    }
    throughput = 1
  }

  http-blocking-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 32
    }
    throughput = 1
  }

}



akka.persistence {
  journal {
    plugin = "akka.persistence.journal.inmem"
    #plugin = "akka-persistence-redis.journal"
  }
  snapshot-store {
    plugin = "akka.persistence.snapshot-store.local"
    #plugin = "akka-persistence-redis.snapshot"
  }
}


akka.http.server.preview.enable-http2 = on
akka.http.server.max-connections = 1024
akka.http.server.pipelining-limit = 16

akka.discovery {
  # Set the following in your application.conf if you want to use this discovery mechanism:
  # method = kubernetes-api

  kubernetes-api {
    class = tech.rocketim.im.kubernetes.KubernetesApiServiceDiscovery

    # API server, cert and token information. Currently these are present on K8s versions: 1.6, 1.7, 1.8, and perhaps more
    api-ca-path = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    api-token-path = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    api-service-host-env-name = "KUBERNETES_SERVICE_HOST"
    api-service-port-env-name = "KUBERNETES_SERVICE_PORT"

    # Namespace discovery path
    #
    # If this path doesn't exist, the namespace will default to "default".
    pod-namespace-path = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"

    # Namespace to query for pods.
    #
    # Set this value to a specific string to override discovering the namespace using pod-namespace-path.
    pod-namespace = "<pod-namespace>"

    # Domain of the k8s cluster
    pod-domain = "cluster.local"

    # Selector value to query pod API with.
    # `%s` will be replaced with the configured effective name, which defaults to the actor system name
    pod-label-selector = "app=%s"
  }
}


akka.kafka.consumer {
  # Config path of Akka Discovery method
  # "akka.discovery" to use the Akka Discovery method configured for the ActorSystem
  discovery-method = akka.discovery

  # Set a service name for use with Akka Discovery
  # https://doc.akka.io/docs/alpakka-kafka/current/discovery.html
  service-name = ""

  # Timeout for getting a reply from the discovery-method lookup
  resolve-timeout = 3 seconds

  # Tuning property of scheduled polls.
  # Controls the interval from one scheduled poll to the next.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that the thread that
  # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
  poll-timeout = 50ms

  # The stage will delay stopping the internal actor to allow processing of
  # messages already in the stream (required for successful committing).
  # This can be set to 0 for streams using `DrainingControl`.
  stop-timeout = 30s

  # Duration to wait for `KafkaConsumer.close` to finish.
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `CommitTimeoutException`.
  # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
  # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
  commit-timeout = 15s

  # If commits take longer than this time a warning is logged
  commit-time-warning = 1s

  # Not relevant for Kafka after version 2.1.0.
  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
  commit-refresh-interval = infinite

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }

  # Time to wait for pending requests when a partition is closed
  wait-close-partition = 500ms

  # Limits the query to Kafka for a topic's position
  position-timeout = 5s

  # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
  # call to Kafka's API
  offset-for-times-timeout = 5s

  # Timeout for akka.kafka.Metadata requests
  # This value is used instead of Kafka's default from `default.api.timeout.ms`
  # which is 1 minute.
  metadata-request-timeout = 5s

  # Interval for checking that transaction was completed before closing the consumer.
  # Used in the transactional flow for exactly-once-semantics processing.
  eos-draining-check-interval = 30ms

  # Issue warnings when a call to a partition assignment handler method takes
  # longer than this.
  partition-handler-warning = 5s

  # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
  # configured by `consumer.metadata-request-timeout`
  connection-checker {

    #Flag to turn on connection checker
    enable = false

    # Amount of attempts to be performed after a first connection failure occurs
    # Required, non-negative integer
    max-retries = 3

    # Interval for the connection check. Used as the base for exponential retry.
    check-interval = 15s

    # Check interval multiplier for backoff interval
    # Required, positive number
    backoff-factor = 2.0
  }

}

#include "persistence.conf"
#include "serializer.conf"

kamon.environment.tags {
  env = staging
  cluster = "cluster"
}

kamon.prometheus {
  start-embedded-http-server = yes
  include-environment-tags = true
  embedded-server {
    hostname = 0.0.0.0
    port = 9505
    //port = ${?KAMON_PORT}
  }

  metric-overrides {
    // example:
    //  "span.processing-time" {
    //    name = new-name
    //
    //    delete-tags = [
    //      tag-name
    //    ]
    //
    //    rename-tags {
    //      "tag-name" = new-tag-name
    //    }
    //  }
  }
}

kamon.prometheus.buckets {
  default-buckets = [
    10,
    30,
    100,
    300,
    1000,
    3000,
    10000,
    30000,
    100000
  ]

  time-buckets = [
    0.005,
    0.01,
    0.025,
    0.05,
    0.075,
    0.1,
    0.25,
    0.5,
    0.75,
    1,
    2.5,
    5,
    7.5,
    10
  ]

  information-buckets = [
    512,
    1024,
    2048,
    4096,
    16384,
    65536,
    524288,
    1048576
  ]
}

kamon.instrumentation.akka.filters {

  actors.track {
    includes = [ "cluster/user/*", "cluster/system/*"]
    excludes = []
  }

  dispatchers {
    includes = [ "cluster/akka.actor.default-dispatcher" ]
  }

  routers {
    includes = [ "cluster/user/some-router" ]
  }
}


kanela.modules {
  executor-service {
    exclude += "scala.concurrent.impl.*"
  }

  scala-future {
    enabled = true
  }

  akka-http {
    instrumentations += "kamon.instrumentation.akka.http.FastFutureInstrumentation"
  }
}

