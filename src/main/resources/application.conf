akka {
  stdout-loglevel = "INFO"
  loglevel = "INFO"
  log-dead-letters = on

  actor {
    # remote
    # cluster
    provider = cluster

    enable-additional-serialization-bindings = on
    allow-java-serialization = off

    serializers {
      kryo = "com.twitter.chill.akka.AkkaSerializer"
    }

    serialization-bindings {
      "java.io.Serializable" = kryo
    }

  }

  cluster {
    min-nr-of-members = 1
    log-info = on
    shutdown-after-unsuccessful-join-seed-nodes = 60s
    split-brain-resolver {
      active-strategy = keep-majority
      stable-after = 10s
    }
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  coordinated-shutdown.exit-jvm = on

  remote {
    maximum-payload-bytes = 30000000 bytes
    netty.tcp {
      message-frame-size = 30000000b
      send-buffer-size = 30000000b
      receive-buffer-size = 30000000b
      maximum-frame-size = 30000000b
    }

    artery {
      canonical.hostname = "127.0.0.1"
      canonical.port = 25520
      advanced {
        maximum-frame-size = 512KiB
        buffer-pool-size = 128
        maximum-large-frame-size = 4MiB
        large-buffer-pool-size = 32
      }
    }
  }


  #extensions = [akka.persistence.Persistence]
  #persistence {

    #journal {
      #plugin = "akka.persistence.journal.leveldb"
      #dir = "/Users/yxwang/dev/my/leveldb"
      #auto-start-journals = ["akka.persistence.journal.leveldb"]
    #}

    #snapshot-store {
      #plugin = "akka.persistence.snapshot-store.local"
      #auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    #}

  #}
}


akka.actor {
    default-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 8
        parallelism-factor = 24.0
        parallelism-max = 32
      }
      shutdown-timeout = 1s
      throughput = 5
    }
}

#akka.stream.materializer.max-input-buffer-size = 32

akka.log-config-on-start = off

# round-robin-group
# round-robin-pool
# balancing-pool

akka.actor {
  deployment {
    /balance-pool-router {
      router = round-robin-pool
      nr-of-instances = 3
      pool-dispatcher {
        executor = "fork-join-executor"
        # Configuration for the fork join pool
        fork-join-executor {
          # Min number of threads to cap factor-based parallelism number to
          parallelism-min = 3
          # Parallelism (threads) ... ceil(available processors * factor)
          parallelism-factor = 2.0
          # Max number of threads to cap factor-based parallelism number to
          parallelism-max = 3
        }
        # Throughput defines the maximum number of messages to be
        # processed per actor before the thread jumps to the next actor.
        # Set to 1 for as fair as possible.
        throughput = 1
      }
    }
  }
}

prio-dispatcher {
  mailbox-type = "tech.parasol.akka.workshop.mailbox.MyPrioMailbox"
}

custom-dispatcher {
  mailbox-requirement = "tech.parasol.akka.workshop.mailbox.MyUnboundedMessageQueueSemantics"
}

custom-dispatcher-mailbox {
  mailbox-type = "tech.parasol.akka.workshop.mailbox.MyPrioMailbox"
}

akka.actor.deployment {
  /myactor {
    dispatcher = custom-dispatcher
  }
}


# akka.http.server.websocket.periodic-keep-alive-max-idle = 10 second
# akka.cluster.sharding.remember-entities = on
# 1 s 1 m 1 h 1 d
# akka.cluster.sharding.passivate-idle-entity-after = 2 m
akka.cluster.sharding.passivate-idle-entity-after = 168 hours

executor {

  writer-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 2
      core-pool-size-factor = 2.0
      core-pool-size-max = 30
    }
    throughput = 1
  }

  forkjoin-writer-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 20  # CPU BOUNDED  <=> IO
      parallelism-max = 24 # Min(parallelism-max, parallelism-factor * Core Count)
    }
    throughput = 1
  }

  pinned-writer-dispatcher {
    type = PinnedDispatcher
  }



  persistence-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 4
      parallelism-factor = 1
      parallelism-max = 32
    }
    throughput = 100
  }

  service-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 8
      parallelism-factor = 24.0
      parallelism-max = 64
    }
    throughput = 1
  }

  http-blocking-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 32
    }
    throughput = 1
  }

}

#akka.http.server.preview.enable-http2 = on
#akka.http.server.max-connections = 1024
#akka.http.server.pipelining-limit = 16




akka.http {
  server {
    max-connections = 4096
  }
}

akka.http {
  client {
    # The default value of the `User-Agent` header to produce if no
    # explicit `User-Agent`-header was included in a request.
    # If this value is the empty string and no header was included in
    # the request, no `User-Agent` header will be rendered at all.
    user-agent-header = akka-http/10.1.14

    # The time period within which the TCP connecting process must be completed.
    connecting-timeout = 10s

    # The time after which an idle connection will be automatically closed.
    # Set to `infinite` to completely disable idle timeouts.
    idle-timeout = 60 s

    # The initial size of the buffer to render the request headers in.
    # Can be used for fine-tuning request rendering performance but probably
    # doesn't have to be fiddled with in most applications.
    request-header-size-hint = 512

    # Socket options to set for the listening socket. If a setting is left
    # undefined, it will use whatever the default on the system is.
    socket-options {
      so-receive-buffer-size = undefined
      so-send-buffer-size = undefined
      so-reuse-address = undefined
      so-traffic-class = undefined
      tcp-keep-alive = undefined
      tcp-oob-inline = undefined
      tcp-no-delay = undefined
    }

    # Client https proxy options. When using ClientTransport.httpsProxy() with or without credentials,
    # host/port must be either passed explicitly or set here. If a host is not set, the proxy will not be used.
    proxy {
      https {
        host = ""
        port = 443
      }
    }

    # Modify to tweak parsing settings on the client-side only.
    parsing {
      # no overrides by default, see `akka.http.parsing` for default values
    }

    # Enables/disables the logging of unencrypted HTTP traffic to and from the HTTP
    # client for debugging reasons.
    #
    # Note: Use with care. Logging of unencrypted data traffic may expose secret data.
    #
    # Incoming and outgoing traffic will be logged in hexdump format. To enable logging,
    # specify the number of bytes to log per chunk of data (the actual chunking depends
    # on implementation details and networking conditions and should be treated as
    # arbitrary).
    #
    # For logging on the server side, see akka.http.server.log-unencrypted-network-bytes.
    #
    # `off` : no log messages are produced
    # Int   : determines how many bytes should be logged per data chunk
    log-unencrypted-network-bytes = off

    websocket {
      # periodic keep alive may be implemented using by sending Ping frames
      # upon which the other side is expected to reply with a Pong frame,
      # or by sending a Pong frame, which serves as unidirectional heartbeat.
      # Valid values:
      #   ping - default, for bi-directional ping/pong keep-alive heartbeating
      #   pong - for uni-directional pong keep-alive heartbeating
      #
      # See https://tools.ietf.org/html/rfc6455#section-5.5.2
      # and https://tools.ietf.org/html/rfc6455#section-5.5.3 for more information
      periodic-keep-alive-mode = ping

      # Interval for sending periodic keep-alives
      # The frame sent will be the onne configured in akka.http.server.websocket.periodic-keep-alive-mode
      # `infinite` by default, or a duration that is the max idle interval after which an keep-alive frame should be sent
      periodic-keep-alive-max-idle = infinite
    }
  }

  host-connection-pool {
    # The maximum number of parallel connections that a connection pool to a
    # single host endpoint is allowed to establish. Must be greater than zero.
    max-connections = 1024

    # The minimum number of parallel connections that a pool should keep alive ("hot").
    # If the number of connections is falling below the given threshold, new ones are being spawned.
    # You can use this setting to build a hot pool of "always on" connections.
    # Default is 0, meaning there might be no active connection at given moment.
    # Keep in mind that `min-connections` should be smaller than `max-connections` or equal
    min-connections = 0

    # The maximum number of times failed requests are attempted again,
    # (if the request can be safely retried) before giving up and returning an error.
    # Set to zero to completely disable request retries.
    max-retries = 5

    # The maximum number of open requests accepted into the pool across all
    # materializations of any of its client flows.
    # Protects against (accidentally) overloading a single pool with too many client flow materializations.
    # Note that with N concurrent materializations the max number of open request in the pool
    # will never exceed N * max-connections * pipelining-limit.
    # Must be a power of 2 and > 0!
    max-open-requests = 4096

    # The maximum duration for a connection to be kept alive
    # This amount gets modified by a 10 percent fuzzyness to avoid the simultanous reconnections
    # defaults to 'infinite'
    # Note that this is only implemented in the new host connection pool
    max-connection-lifetime = infinite

    # Client-side pipelining is not currently supported. See https://github.com/akka/akka-http/issues/32
    pipelining-limit = 1

    # The minimum duration to backoff new connection attempts after the previous connection attempt failed.
    #
    # The pool uses an exponential randomized backoff scheme. After the first failure, the next attempt will only be
    # tried after a random duration between the base connection backoff and twice the base connection backoff. If that
    # attempt fails as well, the next attempt will be delayed by twice that amount. The total delay is capped using the
    # `max-connection-backoff` setting.
    #
    # The backoff applies for the complete pool. I.e. after one failed connection attempt, further connection attempts
    # to that host will backoff for all connections of the pool. After the service recovered, connections will come out
    # of backoff one by one due to the random extra backoff time. This is to avoid overloading just recently recovered
    # services with new connections ("thundering herd").
    #
    # Example: base-connection-backoff = 100ms, max-connection-backoff = 10 seconds
    #   - After 1st failure, backoff somewhere between 100ms and 200ms
    #   - After 2nd, between  200ms and  400ms
    #   - After 3rd, between  200ms and  400ms
    #   - After 4th, between  400ms and  800ms
    #   - After 5th, between  800ms and 1600ms
    #   - After 6th, between 1600ms and 3200ms
    #   - After 7th, between 3200ms and 6400ms
    #   - After 8th, between 5000ms and 10 seconds (max capped by max-connection-backoff, min by half of that)
    #   - After 9th, etc., stays between 5000ms and 10 seconds
    #
    # This setting only applies to the new pool implementation and is ignored for the legacy one.
    base-connection-backoff = 100ms

    # Maximum backoff duration between failed connection attempts. For more information see the above comment for the
    # `base-connection-backoff` setting.
    #
    # This setting only applies to the new pool implementation and is ignored for the legacy one.
    max-connection-backoff = 2 min

    # The time after which an idle connection pool (without pending requests)
    # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
    idle-timeout = 30 s

    # The pool implementation to use. Currently supported are:
    #  - legacy: the original 10.0.x pool implementation
    #  - new: the pool implementation that became the default in 10.1.x and will receive fixes and new features
    pool-implementation = new

    # The "new" pool implementation will fail a connection early and clear the slot if a response entity was not
    # subscribed during the given time period after the response was dispatched. In busy systems the timeout might be
    # too tight if a response is not picked up quick enough after it was dispatched by the pool.
    response-entity-subscription-timeout = 1.second

    # Modify this section to tweak client settings only for host connection pools APIs like `Http().superPool` or
    # `Http().singleRequest`.
    client = {
      # no overrides by default, see `akka.http.client` for default values
    }
  }
}


akka.kafka.consumer {
  # Config path of Akka Discovery method
  # "akka.discovery" to use the Akka Discovery method configured for the ActorSystem
  discovery-method = akka.discovery

  # Set a service name for use with Akka Discovery
  # https://doc.akka.io/docs/alpakka-kafka/current/discovery.html
  service-name = ""

  # Timeout for getting a reply from the discovery-method lookup
  resolve-timeout = 3 seconds

  # Tuning property of scheduled polls.
  # Controls the interval from one scheduled poll to the next.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that the thread that
  # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
  poll-timeout = 50ms

  # The stage will delay stopping the internal actor to allow processing of
  # messages already in the stream (required for successful committing).
  # This can be set to 0 for streams using `DrainingControl`.
  stop-timeout = 30s

  # Duration to wait for `KafkaConsumer.close` to finish.
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `CommitTimeoutException`.
  # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
  # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
  commit-timeout = 15s

  # If commits take longer than this time a warning is logged
  commit-time-warning = 1s

  # Not relevant for Kafka after version 2.1.0.
  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
  commit-refresh-interval = infinite

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }

  # Time to wait for pending requests when a partition is closed
  wait-close-partition = 500ms

  # Limits the query to Kafka for a topic's position
  position-timeout = 5s

  # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
  # call to Kafka's API
  offset-for-times-timeout = 5s

  # Timeout for akka.kafka.Metadata requests
  # This value is used instead of Kafka's default from `default.api.timeout.ms`
  # which is 1 minute.
  metadata-request-timeout = 5s

  # Interval for checking that transaction was completed before closing the consumer.
  # Used in the transactional flow for exactly-once-semantics processing.
  eos-draining-check-interval = 30ms

  # Issue warnings when a call to a partition assignment handler method takes
  # longer than this.
  partition-handler-warning = 5s

  # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
  # configured by `consumer.metadata-request-timeout`
  connection-checker {

    #Flag to turn on connection checker
    enable = false

    # Amount of attempts to be performed after a first connection failure occurs
    # Required, non-negative integer
    max-retries = 3

    # Interval for the connection check. Used as the base for exponential retry.
    check-interval = 15s

    # Check interval multiplier for backoff interval
    # Required, positive number
    backoff-factor = 2.0
  }

}

#include "persistence.conf"
#include "serializer.conf"

kamon.environment.tags {
  env = staging
  cluster = "cluster"
}

kamon.prometheus {
  start-embedded-http-server = yes
  include-environment-tags = true
  embedded-server {
    hostname = 0.0.0.0
    port = 9505
    //port = ${?KAMON_PORT}
  }

  metric-overrides {
    // example:
    //  "span.processing-time" {
    //    name = new-name
    //
    //    delete-tags = [
    //      tag-name
    //    ]
    //
    //    rename-tags {
    //      "tag-name" = new-tag-name
    //    }
    //  }
  }
}

kamon.prometheus.buckets {
  default-buckets = [
    10,
    30,
    100,
    300,
    1000,
    3000,
    10000,
    30000,
    100000
  ]

  time-buckets = [
    0.005,
    0.01,
    0.025,
    0.05,
    0.075,
    0.1,
    0.25,
    0.5,
    0.75,
    1,
    2.5,
    5,
    7.5,
    10
  ]

  information-buckets = [
    512,
    1024,
    2048,
    4096,
    16384,
    65536,
    524288,
    1048576
  ]
}

kamon.instrumentation.akka.filters {

  actors.track {
    includes = [ "cluster/user/*", "cluster/system/*"]
    excludes = []
  }

  dispatchers {
    includes = [ "cluster/akka.actor.default-dispatcher" ]
  }

  routers {
    includes = [ "cluster/user/some-router" ]
  }
}


kanela.modules {
  executor-service {
    exclude += "scala.concurrent.impl.*"
  }

  scala-future {
    enabled = true
  }

  akka-http {
    instrumentations += "kamon.instrumentation.akka.http.FastFutureInstrumentation"
  }
}

