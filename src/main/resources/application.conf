akka {
  stdout-loglevel = "INFO"
  loglevel = "INFO"
  log-dead-letters = on

  actor {
    # remote
    # cluster
    provider = cluster

    enable-additional-serialization-bindings = on
    allow-java-serialization = off

    serializers {
      kryo = "com.twitter.chill.akka.AkkaSerializer"
    }

    serialization-bindings {
      "java.io.Serializable" = kryo
    }

  }

  cluster {
    min-nr-of-members = 1
    log-info = on
    shutdown-after-unsuccessful-join-seed-nodes = 60s
    split-brain-resolver {
      active-strategy = keep-majority
      stable-after = 10s
    }
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  coordinated-shutdown.exit-jvm = on

  remote {
    maximum-payload-bytes = 30000000 bytes
    netty.tcp {
      message-frame-size = 30000000b
      send-buffer-size = 30000000b
      receive-buffer-size = 30000000b
      maximum-frame-size = 30000000b
    }

    artery {
      canonical.hostname = "127.0.0.1"
      canonical.port = 25520
      advanced {
        maximum-frame-size = 512KiB
        buffer-pool-size = 128
        maximum-large-frame-size = 4MiB
        large-buffer-pool-size = 32
      }
    }
  }


  extensions = [akka.persistence.Persistence]

  persistence {

    journal {
      plugin = "akka.persistence.journal.leveldb"
      dir = "/Users/yxwang/dev/my/leveldb"
      auto-start-journals = ["akka.persistence.journal.leveldb"]
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    }

  }
}


akka.stream.materializer.max-input-buffer-size = 32

akka.log-config-on-start = off

# round-robin-group
# round-robin-pool
# balancing-pool

akka.actor {
  deployment {
    /balance-pool-router {
      router = round-robin-pool
      nr-of-instances = 3
      pool-dispatcher {
        executor = "fork-join-executor"
        # Configuration for the fork join pool
        fork-join-executor {
          # Min number of threads to cap factor-based parallelism number to
          parallelism-min = 3
          # Parallelism (threads) ... ceil(available processors * factor)
          parallelism-factor = 2.0
          # Max number of threads to cap factor-based parallelism number to
          parallelism-max = 3
        }
        # Throughput defines the maximum number of messages to be
        # processed per actor before the thread jumps to the next actor.
        # Set to 1 for as fair as possible.
        throughput = 1
      }
    }
  }
}

prio-dispatcher {
  mailbox-type = "tech.parasol.akka.workshop.mailbox.MyPrioMailbox"
}

custom-dispatcher {
  mailbox-requirement = "tech.parasol.akka.workshop.mailbox.MyUnboundedMessageQueueSemantics"
}

custom-dispatcher-mailbox {
  mailbox-type = "tech.parasol.akka.workshop.mailbox.MyPrioMailbox"
}

akka.actor.deployment {
  /myactor {
    dispatcher = custom-dispatcher
  }
}


# akka.http.server.websocket.periodic-keep-alive-max-idle = 10 second
# akka.cluster.sharding.remember-entities = on
# 1 s 1 m 1 h 1 d
# akka.cluster.sharding.passivate-idle-entity-after = 2 m
akka.cluster.sharding.passivate-idle-entity-after = 168 hours

executor {

  writer-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      core-pool-size-min = 2
      core-pool-size-factor = 2.0
      core-pool-size-max = 30
    }
    throughput = 1
  }

  forkjoin-writer-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 20  # CPU BOUNDED  <=> IO
      parallelism-max = 24 # Min(parallelism-max, parallelism-factor * Core Count)
    }
    throughput = 1
  }

  pinned-writer-dispatcher {
    type = PinnedDispatcher
  }



  persistence-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 4
      parallelism-factor = 1
      parallelism-max = 32
    }
    throughput = 100
  }

  service-executor {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 8
      parallelism-factor = 24.0
      parallelism-max = 64
    }
    throughput = 1
  }

  http-blocking-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 32
    }
    throughput = 1
  }

}

akka.http.server.preview.enable-http2 = on
akka.http.server.max-connections = 1024
akka.http.server.pipelining-limit = 16

#akka.persistence.journal.plugin = "akka.persistence.journal.leveldb"
#akka.persistence.snapshot-store.plugin = "akka.persistence.snapshot-store.local"
#akka.persistence.journal.leveldb.dir = "log/journal"
#akka.persistence.snapshot-store.local.dir = "log/snapshots"
# DO NOT USE THIS IN PRODUCTION !!!
# See also https://github.com/typesafehub/activator/issues/287
#akka.persistence.journal.leveldb.native = false


akka {
  persistence {
    journal {
      plugin = "jdbc-journal"
      # Enable the line below to automatically start the journal when the actor system is started
      # auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      plugin = "jdbc-snapshot-store"
      # Enable the line below to automatically start the snapshot-store when the actor system is started
      # auto-start-snapshot-stores = ["jdbc-snapshot-store"]
    }
  }
}

akka-persistence-jdbc {
  shared-databases {
    slick {
      # profile = "slick.jdbc.PostgresProfile$"
      profile = "slick.jdbc.MySQLProfile$"
      db {
        host = "127.0.0.1"
        host = ${?DB_HOST}
        # url = "jdbc:postgresql://"${akka-persistence-jdbc.shared-databases.slick.db.host}":5432/docker?reWriteBatchedInserts=true"
        url = "jdbc:mysql://"${akka-persistence-jdbc.shared-databases.slick.db.host}":3306/persistence?cachePrepStmts=true&cacheCallableStmts=true&cacheServerConfiguration=true&useLocalSessionState=true&elideSetAutoCommits=true&alwaysSendSetIsolation=false&enableQueryTimeouts=false&connectionAttributes=none&verifyServerCertificate=false&useSSL=false&allowPublicKeyRetrieval=true&useUnicode=true&useLegacyDatetimeCode=false&serverTimezone=UTC&rewriteBatchedStatements=true"
        user = "root"
        password = "wyx1005"
        # driver = "org.postgresql.Driver"
        driver = "com.mysql.cj.jdbc.Driver"
        numThreads = 5
        maxConnections = 5
        minConnections = 1
      }
    }
  }
}

jdbc-journal {
  use-shared-db = "slick"
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  use-shared-db = "slick"
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  use-shared-db = "slick"
}

# the akka-persistence-jdbc provider in use for durable state store
jdbc-durable-state-store {
  use-shared-db = "slick"
}



akka.kafka.consumer {
  # Config path of Akka Discovery method
  # "akka.discovery" to use the Akka Discovery method configured for the ActorSystem
  discovery-method = akka.discovery

  # Set a service name for use with Akka Discovery
  # https://doc.akka.io/docs/alpakka-kafka/current/discovery.html
  service-name = ""

  # Timeout for getting a reply from the discovery-method lookup
  resolve-timeout = 3 seconds

  # Tuning property of scheduled polls.
  # Controls the interval from one scheduled poll to the next.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that the thread that
  # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
  poll-timeout = 50ms

  # The stage will delay stopping the internal actor to allow processing of
  # messages already in the stream (required for successful committing).
  # This can be set to 0 for streams using `DrainingControl`.
  stop-timeout = 30s

  # Duration to wait for `KafkaConsumer.close` to finish.
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `CommitTimeoutException`.
  # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
  # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
  commit-timeout = 15s

  # If commits take longer than this time a warning is logged
  commit-time-warning = 1s

  # Not relevant for Kafka after version 2.1.0.
  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
  commit-refresh-interval = infinite

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }

  # Time to wait for pending requests when a partition is closed
  wait-close-partition = 500ms

  # Limits the query to Kafka for a topic's position
  position-timeout = 5s

  # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
  # call to Kafka's API
  offset-for-times-timeout = 5s

  # Timeout for akka.kafka.Metadata requests
  # This value is used instead of Kafka's default from `default.api.timeout.ms`
  # which is 1 minute.
  metadata-request-timeout = 5s

  # Interval for checking that transaction was completed before closing the consumer.
  # Used in the transactional flow for exactly-once-semantics processing.
  eos-draining-check-interval = 30ms

  # Issue warnings when a call to a partition assignment handler method takes
  # longer than this.
  partition-handler-warning = 5s

  # Settings for checking the connection to the Kafka broker. Connection checking uses `listTopics` requests with the timeout
  # configured by `consumer.metadata-request-timeout`
  connection-checker {

    #Flag to turn on connection checker
    enable = false

    # Amount of attempts to be performed after a first connection failure occurs
    # Required, non-negative integer
    max-retries = 3

    # Interval for the connection check. Used as the base for exponential retry.
    check-interval = 15s

    # Check interval multiplier for backoff interval
    # Required, positive number
    backoff-factor = 2.0
  }

}

#include "persistence.conf"
#include "serializer.conf"

kamon.environment.tags {
  env = staging
  cluster = "cluster"
}

kamon.prometheus {
  start-embedded-http-server = yes
  include-environment-tags = true
  embedded-server {
    hostname = 0.0.0.0
    port = 9505
    //port = ${?KAMON_PORT}
  }

  metric-overrides {
    // example:
    //  "span.processing-time" {
    //    name = new-name
    //
    //    delete-tags = [
    //      tag-name
    //    ]
    //
    //    rename-tags {
    //      "tag-name" = new-tag-name
    //    }
    //  }
  }
}

kamon.prometheus.buckets {
  default-buckets = [
    10,
    30,
    100,
    300,
    1000,
    3000,
    10000,
    30000,
    100000
  ]

  time-buckets = [
    0.005,
    0.01,
    0.025,
    0.05,
    0.075,
    0.1,
    0.25,
    0.5,
    0.75,
    1,
    2.5,
    5,
    7.5,
    10
  ]

  information-buckets = [
    512,
    1024,
    2048,
    4096,
    16384,
    65536,
    524288,
    1048576
  ]
}

kamon.instrumentation.akka.filters {

  actors.track {
    includes = [ "cluster/user/*", "cluster/system/*"]
    excludes = []
  }

  dispatchers {
    includes = [ "cluster/akka.actor.default-dispatcher" ]
  }

  routers {
    includes = [ "cluster/user/some-router" ]
  }
}


kanela.modules {
  executor-service {
    exclude += "scala.concurrent.impl.*"
  }

  scala-future {
    enabled = true
  }

  akka-http {
    instrumentations += "kamon.instrumentation.akka.http.FastFutureInstrumentation"
  }
}

